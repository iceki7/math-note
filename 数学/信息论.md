​
**一些定义**
假设两个随机变量分别为X,Y。简记：为p(y)

我们想知道当已知X时，对我们判断Y提供了多少帮助，也就是当已知X=x时，Y=y的概率相比原来增大了多少：
$\frac{p(y|x)}{p(y)}$，
​​​​即 $\frac{p(yx)}{p(x)p(y)}$ 是多少。

对样本(x,y)定义自信息、互信息：
$
\I(x_i):-\log p(x_i)
$
$
\ I(x;y):\log\frac{p(xy)}{p(x)p(y)}
$


对一个概率空间X定义测度：

1、整个空间上：

互信息: $ I(Y;X)=E_0(I(y;x))$

熵 :$H(X)=E_0(I(x))$

条件熵：$H(Y/X)=E(H(Y/x))=\sum\sum p(x_iy_j)\log p(y_j/x_i)$

概率子空间上的互信息：
$I(Y/x):E_x(I_0(y;x))$


**运算**

A与B独立时，$I(ab)=I(a)+I(b)$

$H(Y/X)\leq H(Y)$，X、Y独立时取等。

$H(XY)=H(X)+H(Y|X)$，X,Y统计独立时，$H(Y|X)=H(Y)$

对一个样本的测度：

对整个概率空间的测度：

熵增原理：当X的某个概率P(x=xi)分裂成更多取值时，熵增加。


​
---

wordle和信息论

在wordle的词库，n个词，里随机选一个词，现在如果我们不知道任何“信息”，那么自然每个词概率相同

如果我们进行一次猜测g，相当于就缩小了空间大小。
所以，衡量g质量的方法，就是去看它一次能缩小多少空间。
给出g，它反馈一个颜色组c(g)，n'=n'(c(g))表示缩小后能匹配多少个词。
g不变时，一个词只会对应一个c，
所以出现c的概率就是对应空间的大小。p(c)=n'/n
给一个g，它会随机返回c，出现各种c的概率不同
我们想找一个g，使得p(c)越小越好。
有一些g，它们对应的p(c)相差不大，但也有一些g对应的p(c)恰好相反，而且\sum p_g(c)=1。如果我们定义每一个c中蕴含的“信息有所不同”，我们就是想找一个g，使得c包含的信息的期望最大。Σ p something。
如果现在已经有了c，那么它包含的信息量和n'之间应该是一个单调递减的函数i(n'),这样才符合我们对【信息】这个概念的设想：它越是精确地锁定了某些词，它的信息量就越大。
那么，i=-n'？显然不行。因为这样n'/n, 2n'/2n所对应的信息量就不同，不符合直觉。i=-p(c)似乎更好

但还可以改。如果我们希望给n'中每个字符都排一个01串序号，那么信息还可以理解为是这个01串的长度，即我们要用多长的序列才能够描述可能性的尺寸大小。

也就是说，如果c，c'对应的空间大小为16，8，用于编码可能情况的数据只需要增加一位。——8个可能单词相对于16个的信息量的增加，与4个可能的单词相对于8个的信息量的增加，没有区别。

---

信息量和压缩

        如果压缩是无损的，即通过解压缩可以百分之百地恢复初始的消息内容，那么压缩后的消息携带的【信息】和未压缩的原始【信息】是一样的多。
        
            这个说法符合我们的直觉。所以问题来到了怎么量化它们。


        而压缩后的消息可以通过较少的比特传递，因此压缩消息的每个比特能携带更多的信息。
        
        也就是说压缩信息的熵更加高。熵更高意味着比较难于预测压缩消息携带的信息，原因在于压缩消息里面没有冗余，即每个比特的消息携带了一个比特的信息。香农的信源编码定理揭示了，任何无损压缩技术不可能让一比特的消息携带超过一比特的信息。消息的熵乘以消息的长度决定了消息可以携带多少信息。

-
    比如外观数列和RLE压缩
    1,11,21,1211

    它是无损的。考虑序列1111 1111 6111 1222
    它压缩以后是：81 16 41 32
    编码它的比特虽然变短了。但是它们携带有相同的信息。