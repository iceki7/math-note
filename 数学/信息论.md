​

假设两个随机变量分别为X,Y。
我们想知道，当已知X时，对我们判断Y提供了多少帮助，也就是当已知X=x时，Y=y的概率相比原来增大了多少：
$\frac{p(y|x)}{p(y)}=\frac{p(yx)}{p(x)p(y)}$ 

自信息、互信息：
$
I(x_i)=-\log p(x_i)
\\
\ I(x;y)=\log\frac{p(xy)}{p(x)p(y)}
$


对一个概率空间X定义测度：

1、整个空间上：

互信息: $ I(Y;X)=E_0(I(y;x))$

熵 :$H(X)=E_0(I(x))$

条件熵：$H(Y/X)=E(H(Y/x))=\sum\sum p(x_iy_j)\log p(y_j/x_i)$

概率子空间上的互信息：
$I(Y/x):E_x(I_0(y;x))$


**运算**

A与B独立时，$I(ab)=I(a)+I(b)$

$H(Y/X)\leq H(Y)$，X、Y独立时取等。

$H(XY)=H(X)+H(Y|X)$，X,Y统计独立时，$H(Y|X)=H(Y)$

对一个样本的测度：

对整个概率空间的测度：

熵增原理：当X的某个概率P(x=xi)分裂成更多取值时，熵增加。


​
---

**wordle**

在wordle的词库，n个词里随机选一个词。

如果进行一次猜测g，相当于就缩小了空间大小。
所以，衡量g质量的方法，就是去看它一次能缩小多少空间。
给出g，它反馈一个颜色组C(g)，$\mathbb{W}$表示缩小后尚不能排除的词汇集合，它的大小是$n'=n'(c(g))$。显然C是一个随机变量。

g不变时（比如g=weary），一个词（比如nerdy）只会对应一个c.（它的c是：灰绿灰黄绿），又比如zxcvb对应的是全灰。

如果取一个全部都是常见字母的词组？比如g=aeiou 如果c=全灰，显然符合这个描述的单词是非常少的，所以n_c很小。但是问题在于，如果真的取g=aeiou，全灰的概率本身也是非常小的。因为n_c/n很小）

词是从n个里抽一个。而给定g后，所有单词都映射到唯一的c，所以出现c的概率就是n'所占的大小。
$$p(C=c)=n_c/n$$

给一个g，它会随机返回c，出现各种c的概率不同
我们想找一个g，使得$E_g(n_C)$越小越好。



$E_g(n_C)= \sum_c p(C=c)n_c= \sum p^2(C=c)n$

现在对于C，它的随机变量的取值个数都是固定的（3^5个）
比如：
1/2,1/4,1/4，E=3/8，或1/3，1/3，1/3  E=1/3，
后者E更小。
解释就是：前者有1/2的概率空间排除1/2的词汇，有1/4的概率排除3/4的词汇。还有1/4概率也是排除3/4的词汇。
所以它不能排除的词汇的比例的期望就是3/8。

假设存在一个很小的概率，比如aeiou这种情况：
1/2,1/2-1/100,1/100，s~=1/4+1/4=1/2，它不如

讨论性质。
**假设如果把一个概率分布分裂成2个**
1/2,1/2→1/2,1/4，1/4
由于(a+b)^2>a2+b2，所以分裂的越多E越小。

**如果C的取值个数不变(k)，猜测概率相等时，E最小。**
$E=n\sum \frac{1}{k^2}$
证明：
假设有一个分布它是不均匀的，比如1/2,1/4,1/4
我们考虑如果把它变得均匀，这个过程E的变化情况。

从多的一块概率(p)里取出一部分d，补充少的(q)

$\Delta E=d^2-2pd+d^2+2dq\\=2d(q-p+d)
，只要q+d<=p，就有\Delta E<0$

所以，这个“劫富济贫”的过程，E始终在下降。
这个形式和熵很像。

其实还可以思考，如果c不是指返回的颜色组，而是对信息的一个编码呢？
那么，词库W就可以看成是所有可能出现的信息字符，比如字母a~z，C是对它重新进行编码映射，对于常见的组合，比如con,tion等，我们单独赋予一个编码，而对于一些不常见的组合，比如qzx，就还是按照原样书写。假设编码是无损的。而且S里若干元素可以组合编码。比如tx是不常见的组合，那么这里的t要单独编码，但是tion就不用。

这样就等于还是划分出了一个空间W

上述我们想优化的东西，可以修改为如何编码才能使得码长的期望最短，那么$E=\sum -p\log_2 p$就很合理了。





上述的讨论有一个错误，那就是并不是所有的c都会有单词映射到。比如g=zxcvb，因为这不是一个正规的单词，所以实际上c=全绿不映射到任何单词上。
也就是说，任何单词都映射到唯一的一个c。但一个c可能对应多个单词，也可能一个都不对应.g=zxcvb时，全绿的概率为零


---
以上只是针对wordle第一次猜测的时候的最优化。因为第一次能把空间尽可能缩小，并不意味着之后继续缩小空间时还能最优化。因为它跟我们第一次缩小到了哪个空间有关系。第二次如果得到的空间是$\mathbb{W}'$
那我们要优化
$$
\min_{\mathbb{W'}} |\mathbb{W}\cap \mathbb{W}'|$$

空间缩小：n'
10→5→3
    →4
10→7→1
    →2
第二次猜测和第一次猜测并没有什么区别。同样是缩小到一个空间$\mathbb{W}'$，但是我们这次取的是$\mathbb{W}'\cap \mathbb{W}$，作为两次猜测得到的信息。
所以，N'的期望最小，并不意味着$N'\cap N''$ 的期望最小


整个worlde优化的是猜测的次数k，



---

**用熵考虑上述问题**


信息是01串。

某信息长度为I，且假设01出现概率相同，那么某个信息出现的概率？
)1/2)


信息是对空间的缩小。假设现在有若干元素，01串用二分法确定了空间里的元素集合。那么一个信息就对应空间中的一个可能性集合。


比如如果有16个元素，给出的信息是01，那么它指的是左下角4个元素中的一个。

所以消息m和$W$一一对应。

因为0，1出现的概率相同，等价于随机在空间里选取元素，
所以log(1/p)=|m|代表了编码那个空间中数目的元素，需要多长的信息。
而p(c=C)刚好是词库里对应的单词数目。
假设我们有1024个单词，每个单词有一个10位的编码。现在，p=1/4，
W的大小是256个单词，所以我们已经得到了编码中的4位。（注意，这个和编码方式无关，4位的意思是，我们必须选取特定的方式，才能让W中空间的词汇对应这4个编码）
编码就相当于信息量，我们得到10个编码时，也就猜出了单词是什么。
那么，对于不同的g，会有一个编码数量的期望，我们希望最大化它$E=\sum p |m_p|=\sum p(-\log p)$。熵。


和前面的方法对比，前者看的是缩小空间的绝对大小。
但这里看的是缩小空间所占的比例。好处是，在之后猜测时候，可以累加。

$$
(1/2)^I=p, (1/2)^J=q

$$





---

信息量和压缩

        如果压缩是无损的，即通过解压缩可以百分之百地恢复初始的消息内容，那么压缩后的消息携带的【信息】和未压缩的原始【信息】是一样的多。
        
            这个说法符合我们的直觉。所以问题来到了怎么量化它们。


        而压缩后的消息可以通过较少的比特传递，因此压缩消息的每个比特能携带更多的信息。
        
        也就是说压缩信息的熵更加高。熵更高意味着比较难于预测压缩消息携带的信息，原因在于压缩消息里面没有冗余，即每个比特的消息携带了一个比特的信息。香农的信源编码定理揭示了，任何无损压缩技术不可能让一比特的消息携带超过一比特的信息。消息的熵乘以消息的长度决定了消息可以携带多少信息。

-
    比如外观数列和RLE压缩
    1,11,21,1211

    它是无损的。考虑序列1111 1111 6111 1222
    它压缩以后是：81 16 41 32
    编码它的比特虽然变短了。但是它们携带有相同的信息。


---
KL散度
$$
\sum_i P(i)\ln \frac{P(i)}{Q(i)}
$$
ln项衡量Q对P的预测，差的越远值越大，前面的p是权重。

**cross entropy**
$$
H(p,q)=\sum p\log q
$$

--- 
**反向思考**

学习信息论的时候通常先给出熵的定义，再根据公式说明它为什么能衡量一个分布的不确定性。
不如换个思路，先考虑：如何衡量一个概率分布的混乱程度，再由此制定一些可行的指标。

比如：有3个概率分布：
A=(0.5,0.3,0.2)
B=(0.5,0.29,0.21)
C=(0.5,0.3,0.19,0.01)

我们希望对分布有一个数值作为指标，衡量它的不确定性。

如果考虑“混乱”、“不确定性”这些词的含义，我们希望指标反映的结果是:

**我们希望混乱程度C>A**，因为C相当于从A的0.2中分出去了0.01。情况的种类变多了。

**我们希望混乱程度：B>A**。因为A相当于从B的0.21中取出了0.01分到了0.29中。原本可能性较大的部分变得“更加有可能”了，原本可能性相对较小的部分变得“更加不可能”了
根据上述的观察，来制定一个计算方式得出某个指标H，以满足这些需求吧。要求：H(C)>H(A)，H(B)>H(A)

先考虑如何使得H(C)>H(A)。

一个简单粗暴的想法是，根据等式$(a+b)^2>a^2+b^2$

如果H是概率的平方和的相反数，那么自然就会有 $H(C)>H(A)$

所以先试试我们发明的这种指标：
$$H=\sum_x -p^2(x)$$

那么，它能使得H(B)>H(A)吗？

为了验证这一点，写出A和B的指标H的差值：
$$
(a-x)^2+(b+x)^2-a^2-b^2=2x(b+x-a)，其中a>b>0,x>0
$$

显然，如果b+x<a ，那么式子为负，则H(B)>H(A)。

P.S. 这个式子用一个形象的比喻来说，就是“劫富济贫”——将a中的0.3分出0.01给0.2的部分。也即，只要“劫富济贫”过后贫富差距是在缩小的，那么指标H就一定是在增大的。


总结上述讨论可以发现：平方和的负数的确可以满足上述两种需求。


当然，大家肯定猜到了，熵也是满足上述需求的。既然如此，熵比起上述指标的优点在哪呢？






