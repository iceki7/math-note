​
**一些定义**
假设两个随机变量分别为X,Y。简记：为p(y)

我们想知道当已知X时，对我们判断Y提供了多少帮助，也就是当已知X=x时，Y=y的概率相比原来增大了多少：
$\frac{p(y|x)}{p(y)}$，
​​​​即 $\frac{p(yx)}{p(x)p(y)}$ 是多少。

对样本(x,y)定义自信息、互信息：
$
\I(x_i):-\log p(x_i)
$
$
\ I(x;y):\log\frac{p(xy)}{p(x)p(y)}
$


对一个概率空间X定义测度：

1、整个空间上：

互信息: $ I(Y;X)=E_0(I(y;x))$

熵 :$H(X)=E_0(I(x))$

条件熵：$H(Y/X)=E(H(Y/x))=\sum\sum p(x_iy_j)\log p(y_j/x_i)$

概率子空间上的互信息：
$I(Y/x):E_x(I_0(y;x))$


**运算**

A与B独立时，$I(ab)=I(a)+I(b)$

$H(Y/X)\leq H(Y)$，X、Y独立时取等。

$H(XY)=H(X)+H(Y|X)$，X,Y统计独立时，$H(Y|X)=H(Y)$

对一个样本的测度：

对整个概率空间的测度：

熵增原理：当X的某个概率P(x=xi)分裂成更多取值时，熵增加。


​