**机器学习的一般形式**

在限定了某个样本空间D（数据集）的情况下，损失函数是参数$\theta$的函数。


$$
L=f(\theta)
$$

在D中取一个测试集和训练集，检查其泛化性。

让参数在网络的负梯度方向上移动以降低损失：
$$
\theta-=\nabla_\theta L
$$

网络是函数的迭代$f(..f()...)$。

为避免局部最小，发明了很多梯度下降的方法。

为捕捉整体特征，提出了批训练的概念。

使用激活函数赋予非线性，并且激活函数也是解析的。

所以根据链式法则，可以写出损失对任何一个参数的导数
$L'_{\theta}$


卷积：捕捉邻域特征。


pooling：改变分辨率。例如图像的分类问题。