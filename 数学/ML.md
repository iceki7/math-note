**机器学习的一般形式**

在限定了某个样本空间D（数据集）的情况下，损失函数是参数$\theta$的函数。


$$
L=L(\hat{y},y),\hat{y}=f(\theta,x)
$$
f代表预测网络。(x,y)是数据集中的点，也就是通过插值这些点，来构建映射关系，实现对未知输入的推测。


在D被限定在某个范围的情况下，L可视为$\theta$的函数。

$$
L=g(\theta)
$$

让参数在负梯度方向上移动以降低损失：
$$
\theta-=\nabla_\theta L
$$


由于网络是函数的迭代$f(..f()...)$，如果其中每一个部分都是解析的，根据链式法则可以写出L对任何一个参数的导数
$L'_{\theta}$


----
为了评估网络在一些没有训练过的点上的准确性，取一些从未输入过的数据$(x_t,y_t)$作为测试集。如果准确率很高，说明网络对于这些数据的插值是足够有效的。此时可以认为网络的参数中存储了空间到目标的映射过程。是有信息的。当然，测试集和训练集是同一类型的数据。


由于网络具有连续性，所以输入的小范围变动也只会引起输出的小范围变动。
这与现实世界人们的认知方式一致。（比如一副数字2的图像，改变其中的一个像素点并不会让人觉得这因此就不是数字2了，想要变成另一个数字需要经过很大范围的变动）。
另一个例子就是AI作画中，可以将一幅画渐变为另外一副。（由此，也可以研究输入对于输出是如何进行控制的，换句话说，输入中存在着语义信息）

使用激活函数赋予非线性的拟合能力.

为避免局部最小，发明了很多梯度下降的方法。

为捕捉整体特征，提出了批训练的概念。也就是$g(\theta)$是关于许多个数据点的平均损失。根据这些平均损失执行梯度回传。所以g实际上不是某个特定的函数（除了全批量训练时，也就是一次性计算所有数据点的平均最低loss，此时的全局最优点理论上就是网络的最优解，但是全批量可能很难找到这个点）

为什么MLP能拟合函数：任意逼近定理。


卷积：捕捉邻域特征。


pooling：改变分辨率。例如图像的分类问题。